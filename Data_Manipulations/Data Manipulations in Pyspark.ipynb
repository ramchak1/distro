{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from filepath on distributed system\n",
    "file_location = \"/FileStore/tables/avocado.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "\n",
    "# Read from existing table on hive\n",
    " df=spark.sql('select * from tablename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glimpse of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rows=df.count()\n",
    "print ('Total data count is '+str(count_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - using lists \n",
    "columns_to_subset=['Total Volume','AveragePrice']\n",
    "df1=df.select(*[columns_to_subset])\n",
    "\n",
    "# Method 2 - Column names\n",
    "df1=df.select('Total Volume','AveragePrice')\n",
    "\n",
    "# Method 3 - Indexes\n",
    "df1=df.select(df[2],df[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df['Total Volume'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One way Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['type']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df['region']=='Albany').crosstab('type','region').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give a quick glimpse to data, if columns are not mentioned all the numeric columns stats are produced \n",
    "columns_to_analyze=['Total Volume','AveragePrice']\n",
    "df.select(*[columns_to_analyze]).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Total Volume',df['Total Volume'].cast(\"float\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median Value Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parameters have to be passed through approxQuantile function\n",
    "#1. col – the name of the numerical column\n",
    "#2. probabilities – a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
    "#3. relativeError – The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but give the same result as 1.\n",
    "\n",
    "median=df.approxQuantile('Total Volume',[0.5],0.1)\n",
    "print ('The median of Total Volume is '+str(median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of distinct levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "column_name='region'\n",
    "count_distinct=df.agg(countDistinct(col(column_name).alias(\"distinct_counts\"))).head()[0]\n",
    "print ('The number of distinct values of '+column_name+ ' is ' +str(count_distinct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinct Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name='type'\n",
    "df.select(column_name).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets filter the organic type avocados within Albany region\n",
    "filtered_count=df.filter((df['type']=='organic') & (df.region=='Albany')).count()\n",
    "print ('subset data count is '+str(filtered_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumnRenamed(\"Total Volume\",\"Total_Volume\")\n",
    "# For multiple columns\n",
    "df=df.withColumnRenamed(\"Total Bags\",\"Total_Bags\").withColumnRenamed(\"Small Bags\",\"Small_Bags\").withColumnRenamed(\"Large Bags\",\"Large_Bags\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using withColumn\n",
    "df1=df.select('Total_Volume','Total_Bags').withColumn('avg_volume_bag',df['Total_Volume']/df['Total_Bags'])\n",
    "\n",
    "# Alternative method \n",
    "df = df.withColumn('Total_Bags',df['Total_Bags'].cast(\"float\"))\n",
    "df1=df.select('_c0','Total_Volume','Total_Bags',(df['Total_Volume']/df['Total_Bags']).alias('avg_volume_bag'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary data types\n",
    "from pyspark.sql.functions import udf,split\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "# Create a function for all the data maipulations\n",
    "def new_cols(Total_Volume,AveragePrice):\n",
    " if Total_Volume<44245: Volume_Category='Small'\n",
    " elif Total_Volume<850644: Volume_Category='Medium'\n",
    " else: Volume_Category='Large'\n",
    " if AveragePrice<1.25: Price_Category='Low'\n",
    " elif AveragePrice<1.4: Price_Category='Mid'\n",
    " else: Price_Category='High'\n",
    " return Volume_Category,Price_Category  \n",
    "\n",
    "# Apply the user defined function on the dataframe \n",
    "\n",
    "udfB=udf(new_cols,StructType([StructField(\"Volume_Category\", StringType(), True),StructField(\"Price_Category\", StringType(), True)]))\n",
    "df2=df.select('_c0','Total_Volume','AveragePrice').withColumn(\"newcat\",udfB(\"Total_Volume\",\"AveragePrice\"))\n",
    "\n",
    "# Unbundle the struct type columns into individual columns and drop the struct type \n",
    "df3 = df2.select('_c0','Total_Volume','AveragePrice','newcat').withColumn('Volume_Category', df2.newcat.getItem('Volume_Category')).withColumn('Price_Category', df2.newcat.getItem('Price_Category')).drop('newcat')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Operations - Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "df3=df3.withColumn('VolumePrice_Category',concat(df3.Volume_Category,df3.Price_Category))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Operations - ChangeCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,trim,upper\n",
    "df3=df3.withColumn('Price_Category',trim(upper(df3.Price_Category)))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update a column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df3.withColumn('Volume_Category',when(df3['Volume_Category']=='Medium','Mid').otherwise(df3['Volume_Category']))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df4.drop('VolumePrice_Category')\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df4.sort(col(\"Total_Volume\").desc())\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as hive table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.saveAsTable('avocado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.format(\"csv\").option(\"delimiter\", \"|\").save('avocado_textfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df=df3.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a montonically increasing id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
    "df5 = df4.withColumn(\"new_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The join will include both keys from the tables. Common key can be explicitly dropped using a drop statement or subset of columns needed after join can be selected\n",
    "# inner, outer, left_outer, right_outer, leftsemi joins are available \n",
    "joined_df = df3.join(df1, df1['_c0'] == df3['_c0'], 'inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
